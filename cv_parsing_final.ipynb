{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a66b7c03",
      "metadata": {
        "id": "a66b7c03"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "547b1676",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "547b1676",
        "outputId": "e6e3ec39-881c-4737-d89d-1d5290995604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99cff7eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99cff7eb",
        "outputId": "dc8a23a2-5f3a-4375-ca67-d51e352daa32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.10.2-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20221105 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.20.0-py3-none-manylinux_2_17_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (3.2.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (41.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
            "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20221105 pdfplumber-0.10.2 pypdfium2-4.20.0\n"
          ]
        }
      ],
      "source": [
        "pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b2d748d",
      "metadata": {
        "id": "6b2d748d"
      },
      "outputs": [],
      "source": [
        "import PyPDF2, pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d733a9b2",
      "metadata": {
        "id": "d733a9b2"
      },
      "outputs": [],
      "source": [
        "CV=\"/content/my_resume-1.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14b3538e",
      "metadata": {
        "id": "14b3538e"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "CV_File=open(CV,'rb')\n",
        "Script=PyPDF2.PdfReader(CV_File)\n",
        "pages=len(Script.pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f568b7b2",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f568b7b2",
        "outputId": "047bee5b-5258-42b8-a71b-eba226ed62c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DigvijaySinghBisht\n",
            "Email-id: digvijay6603@gmail.com\n",
            "MobileNo.: 7457036375,9528997565\n",
            "https://github.com/Digvijay6\n",
            "ACADEMICDETAILS\n",
            "Year Degree/Exam Institute GPA/Percentage\n",
            "Oct,2021-Present B.TECHinComputerScience GraphicEraHillUniversityDehradun\n",
            "2021 12th, I.S.E. St. Joseph’sCollege,Nainital 84.60%\n",
            "2019 10th, I.C.S.E. St. Joseph’sCollege,Nainital 84.20%\n",
            "RESEARCHPUBLICATION\n",
            "• \"A Comparative Analysis of IoT Malware Detection Using CNN and Deep Learning - Developed an\n",
            "efficient algorithm to detect malware in IOT devices. Accepted in the 2nd International Conference on\n",
            "TechnologicalAdvancementsinComputationalSciences”(ICTACS-2023)\n",
            "• Computerized Face Mask Detection Using Deep CNN and Transfer Learning - Proposed a faster and\n",
            "efficient algorithm to detect face mask in public places. Published in: 2023 International Conference on\n",
            "DeviceIntelligence,ComputingandCommunicationTechnologies,(DICCT)\n",
            "PROJECTS\n",
            "• IOTmalwareDetection: DevelopedanartificialneuralnetworkclassifierforrecognizingmalwareinIOT\n",
            "devices.\n",
            "• Brain Tumour Detection : Deep Learning and Neural Network architecture which pre-processes images\n",
            "thedatasetusingvariousimageprocessingmethodsandtrainsadatasetontransferlearningmodelsand\n",
            "tuningtheirvariousparameterstillbestoptimization.\n",
            "• Face Mask Detection : A machine learning architecture which pre-processes images using advanced\n",
            "python libraries and trains a vast dataset on deep learning models, then further trained on the various\n",
            "optimizersofthemodelwithbestaccuracy.\n",
            "• Image Classfication : Classification of various celebrity images, scrapped from all over the internet and\n",
            "pre-processed using haarcascade and wavelet transform, with the help of conventional machine learning\n",
            "approachwhichhadfrontendinterfacelinkedtroughFlaskAPI.\n",
            "TECHNICALSKILLS\n",
            "• Languages: Python(proficient),Java(proficient),C(proficient),C++(proficient),SQL\n",
            "• ToolsandFrameworksMATLAB,FlaskLATEX\n",
            "• MachineLearningToolsTensorflow,Keras,scikit-learn.\n",
            "CERTIFICATIONS\n",
            "• DataScienceOrientationbyIBM\n",
            "• ToolsforDataSciencebyIBM\n",
            "• WhatisDataScience\n",
            "• DataScienceFoundations\n",
            "• AlibabaCloudAIForward\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "Script = []\n",
        "with pdfplumber.open(CV_File) as pdf:\n",
        "    for i in range (0,pages):\n",
        "        page=pdf.pages[i]\n",
        "        text=page.extract_text()\n",
        "        print (text)\n",
        "        Script.append(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea295768",
      "metadata": {
        "id": "ea295768"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a045ac6",
      "metadata": {
        "id": "8a045ac6"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import re #regular expresion\n",
        "resumeText = re.sub('![A-Za-z1-9]', '  ', Script[0])\n",
        "resumeText=re.sub('\\n', '  ', resumeText)\n",
        "\n",
        "email=re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', resumeText)\n",
        "\n",
        "links=re.findall('http\\S+\\s*', resumeText)\n",
        "links2=re.findall('[A-Za-z]*.com',resumeText)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c14bafb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c14bafb",
        "outputId": "61b3dffc-b63c-4568-c421-dd583eb8d89c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DigvijaySinghBisht\n",
            "Email-id: digvijay6603@gmail.com\n",
            "MobileNo.: 7457036375,9528997565\n",
            "https://github.com/Digvijay6\n",
            "ACADEMICDETAILS\n",
            "Oct,2021-Present B.TECHinComputerScience GraphicEraHillUniversityDehradun\n",
            "RESEARCHPUBLICATION\n",
            "TechnologicalAdvancementsinComputationalSciences”(ICTACS-2023)\n",
            "DeviceIntelligence,ComputingandCommunicationTechnologies,(DICCT)\n",
            "PROJECTS\n",
            "• IOTmalwareDetection: DevelopedanartificialneuralnetworkclassifierforrecognizingmalwareinIOT\n",
            "devices.\n",
            "thedatasetusingvariousimageprocessingmethodsandtrainsadatasetontransferlearningmodelsand\n",
            "tuningtheirvariousparameterstillbestoptimization.\n",
            "optimizersofthemodelwithbestaccuracy.\n",
            "approachwhichhadfrontendinterfacelinkedtroughFlaskAPI.\n",
            "TECHNICALSKILLS\n",
            "• Languages: Python(proficient),Java(proficient),C(proficient),C++(proficient),SQL\n",
            "• ToolsandFrameworksMATLAB,FlaskLATEX\n",
            "• MachineLearningToolsTensorflow,Keras,scikit-learn.\n",
            "CERTIFICATIONS\n",
            "• DataScienceOrientationbyIBM\n",
            "• ToolsforDataSciencebyIBM\n",
            "• WhatisDataScience\n",
            "• DataScienceFoundations\n",
            "• AlibabaCloudAIForward\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "sections=[]\n",
        "for i in Script[0].split('\\n'):\n",
        "    if len((i.split())) <= 3 :\n",
        "        print (i)\n",
        "        sections.append(i)\n",
        "Name=sections[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2dda24a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2dda24a",
        "outputId": "cfc318c9-fe13-4a69-9cea-0b8a66c887f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "%%time\n",
        "numeric_data=0\n",
        "for line in Script[0].split('\\n'):\n",
        "    for i in line.split():\n",
        "        if i.isdigit()==True :\n",
        "            numeric_data+=1;\n",
        "numeric_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f97db06",
      "metadata": {
        "id": "6f97db06"
      },
      "outputs": [],
      "source": [
        "headings={'education' :1,'qualification' :1,'project':1,'skills':1,'relevant coursework':1,'achievement':1,'experience':1,'awards and honors':1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa9aa911",
      "metadata": {
        "id": "fa9aa911"
      },
      "outputs": [],
      "source": [
        "for i in sections:\n",
        "     i.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64e08a70",
      "metadata": {
        "id": "64e08a70"
      },
      "source": [
        "# loading Skills from JD and JD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68b9f60b",
      "metadata": {
        "id": "68b9f60b"
      },
      "outputs": [],
      "source": [
        "skills=['hello','Android']\n",
        "JD=\"JOB DESCRIPTION | Data Analyst  Job Title:  Data Analyst   Salary Grade (HR):   job Code:    FLSA Status (HR):  Non-Exempt Department Name:  Information Technology and Data  Approved By (HR):  Reports to (Title): Chief Operations Officer  Date Approved (HR): Date Prepared: October 11, 2016  Prepared By:  Teresa Mooney JOB SUMMARY: Under the supervision of the Chief Operations Officer, this vital role supports HOPES strategic business plan, agency data and reporting initiatives. This position has a specific focus on developing and coordinating systems and processes that engage and solicit data management and reporting analytics across the agency.   ESSENTIAL FUNCTIONS: Leadership | Identify, Evaluate & Utilizes Data | Data Project Management | Training & Development of Data Material Leadership 1.  Ability to capture vision, utilize great communication skills to collaborate and partner with multidisciplinary team members to help launch strategic data driven initiatives. 2.  Work closely with CEO and COO on business plan/ strategic plan 3.  Lead point of contact and subject matter expert for obtaining all data aspects of Community health and needs assessment  Identify, Evaluates & Utilizes Data   Identifies Data: 1.  Knowledgeable of the high level of reporting needed to provide data and capture big picture objectives  2.  Works closely with IT department in building of pertinent data for real time information (Dashboard) & reporting Evaluates Data: 1.  Understands what’s behind automation and entailed in building, revising & troubleshooting the reporting aspects of analytics 2.  Ability to help move forward and lead the Data Analyst metrics for Continuous Quality Improvement (CQI) initiatives  3.  Figure out how to mine information from clinical and operational data and distill best practices, enabling Care Team to create information-driven care plans in real time.  4.  Work closely with IT department to develop on demand data via Electronic Provider boards (Visual Dashboard) 5.  Reporting to improve population health and population medicine Utilizes Data:   1.  Work with PCMH team to submit PCMH application 2.  This entails possessing the knowledge base, to help teams recognize and identify what’s needed (required) to build and set up efficient workflow process   3.  Tracks the impact and results of pilot programs, determining which have the greatest measurable impact. 4.  Reviews process/workflow improvements based upon data collection & analysis  5.  Analyzes and develops Score Card metrics for providers  6.  Evaluates grant reporting requirements, analyzes process to collect data needed in order to produce accurate reporting.  7.  Develop reports to look at trends within agency, to guide programming  Data Project Management 1.  This entails possessing the knowledge base, to help teams recognize and identify what’s needed (required) to build and set up efficient workflow process 2.  Work with PCMH team to submit PCMH application  Training & Development of Data Material 1.  Presentation skills  and helps implement necessary information to Care Team  2.  To include presenting  training materials (FAQ, Fact Sheets, Cheat sheets) to end users that result in staff gaining daily proficiency and being empowered to adapt to ongoing changes     REQUIREMENTS: 1.  2-3 years’ experience in Data Analyst role/Communication with a savvy, enthusiastic and analytical mind; experience in healthcare and nonprofit sector a plus 2.  Ability to easily consume, process and analyze data, as well as to respond with appropriate strategies and tactics 3.  Ability to multitask and meet deadlines 4.  Ability to work autonomously as well as collaboratively, ready to produce results  MARGINAL/ADDITIONAL FUNCTIONS:  1.  Assist with other department activities as assigned 2.  Support other HOPES departments as needed    REPORTING STRUCTURE:   Supervision Received:     Reports to the Chief Operations Officer Supervision Exercised:    None Directly Reporting:      None Indirectly Reporting:     None  CONTACTS: Internal:   HOPES staff, volunteers and patients,  External:   donors, vendors, volunteers, and community  Education or equivalency:  Bachelor’s Degree in Business, Healthcare Management/Administration or 2-5 years’ experience in related field  Experience: 2-3 years’ experience in a Financial Analyst role in Healthcare or Non-profit setting  EMPLOYEE ACKNOWLEDGEMENT \";"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d18f49df",
      "metadata": {
        "id": "d18f49df"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "Req_Clear=JD\n",
        "\n",
        "Match_Test=[resumeText,Req_Clear]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "019d65a6",
      "metadata": {
        "id": "019d65a6"
      },
      "source": [
        "# Similarity BW JD and CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7ad8669",
      "metadata": {
        "id": "e7ad8669"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv=CountVectorizer()\n",
        "count_matrix=cv.fit_transform(Match_Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8541d73e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8541d73e",
        "outputId": "31367bf5-5785-42df-d271-166fd60281db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity is : [[1.         0.32512036]\n",
            " [0.32512036 1.        ]]\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "print('Similarity is :',cosine_similarity(count_matrix))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61aae17e",
      "metadata": {
        "id": "61aae17e"
      },
      "source": [
        "# searching skills"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5147997",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5147997",
        "outputId": "746fb7d8-cd1d-411f-a22b-6baa84a66ddd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "#skills search\n",
        "%%time\n",
        "score=0;\n",
        "for i in skills:\n",
        "    mentions=re.findall(i, resumeText)\n",
        "    score=score+len(mentions)\n",
        "score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53268479",
      "metadata": {
        "id": "53268479"
      },
      "source": [
        "# Gramer Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a025aa18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a025aa18",
        "outputId": "c8d7612a-a5bd-48d4-d9a6-0c1212c97390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2023.7.22)\n",
            "Installing collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.7.1\n"
          ]
        }
      ],
      "source": [
        "pip install language-tool-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53faca21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53faca21",
        "outputId": "7f9cdeef-2fdb-4f02-c6e0-1c055a9a529f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting 3to2\n",
            "  Downloading 3to2-1.1.1.zip (78 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m71.7/78.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: 3to2\n",
            "  Building wheel for 3to2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for 3to2: filename=3to2-1.1.1-py3-none-any.whl size=78772 sha256=9a630ea9a7bfe19c3e78bcc3e0bb035de0bccde10c77333d1db1c5ffbdffacdd\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/a4/21/0953634e21cb8359fbd01488c6c0dfdd2155145b7eee8f3ada\n",
            "Successfully built 3to2\n",
            "Installing collected packages: 3to2\n",
            "Successfully installed 3to2-1.1.1\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade 3to2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df7c48ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df7c48ba",
        "outputId": "21397a07-fe80-4e64-ee68-e8f47163b3c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading LanguageTool 5.7: 100%|██████████| 225M/225M [00:04<00:00, 47.5MB/s]\n",
            "INFO:language_tool_python.download_lt:Unzipping /tmp/tmpyzug22s_.zip to /root/.cache/language_tool_python.\n",
            "INFO:language_tool_python.download_lt:Downloaded https://www.languagetool.org/download/LanguageTool-5.7.zip to /root/.cache/language_tool_python.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of mistakes found in document is  45\n",
            "\n",
            "Offset 2, length 21, Rule ID: MORFOLOGIK_RULE_EN_US\n",
            "Message: Possible spelling mistake found.\n",
            "• AlibabaCloudAIForward\n",
            "  ^^^^^^^^^^^^^^^^^^^^^\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import language_tool_python\n",
        "\n",
        "# Mention the language keyword\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "i = 0\n",
        "\n",
        "# Path of file which needs to be checked\n",
        "\n",
        "for line in Script[0].split('\\n'):\n",
        "    matches = tool.check(line)\n",
        "    i = i + len(matches)\n",
        "# prints total mistakes which are found\n",
        "# from the document\n",
        "\n",
        "print(\"No. of mistakes found in document is \", i)\n",
        "print()\n",
        "\n",
        "# prints mistake one by one\n",
        "for mistake in matches:\n",
        "    print(mistake)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c3e9ac5",
      "metadata": {
        "id": "2c3e9ac5"
      },
      "source": [
        "# Ageing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8463477",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8463477",
        "outputId": "dcf75fd5-dc3f-4012-92aa-89acc95221d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datefinder\n",
            "  Downloading datefinder-0.7.3-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.10/dist-packages (from datefinder) (2023.6.3)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from datefinder) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datefinder) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.16.0)\n",
            "Installing collected packages: datefinder\n",
            "Successfully installed datefinder-0.7.3\n"
          ]
        }
      ],
      "source": [
        "pip install datefinder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea3d555",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ea3d555",
        "outputId": "039ef2e0-3184-460a-aca3-45082a08fabc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current date and time: 2023\n",
            "{}\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from datetime import datetime\n",
        "\n",
        "# Get the current date and time\n",
        "current_datetime = (datetime.now().year)\n",
        "t1=str(current_datetime)\n",
        "t2=str(current_datetime-1)\n",
        "print(\"Current date and time:\", current_datetime)\n",
        "\n",
        "# Formatting a datetime object as a string\n",
        "# formatted_datetime = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "time_SCORE=0;\n",
        "dist={}\n",
        "for line in Script[0].split('\\n'):\n",
        "    f=0\n",
        "    for word in line.split():\n",
        "#     cout<<word\n",
        "        if word==t1:\n",
        "            f=1;\n",
        "    for word in line.split('-'):\n",
        "        if word==t1:\n",
        "            f=1;\n",
        "    for word in line.split('/'):\n",
        "        if word==t1:\n",
        "            f=1;\n",
        "    if f==1:\n",
        "        for word in line.split():\n",
        "            if word in skills:\n",
        "                if word in dist:\n",
        "                    dist[word]=dist[word]+2\n",
        "                else:\n",
        "                    dist[word]=2\n",
        "\n",
        "    f1=0\n",
        "    for word in line.split():\n",
        "        if word==t2:\n",
        "            f1=1;\n",
        "    for word in line.split('-'):\n",
        "        if word==t2:\n",
        "            f1=1;\n",
        "    for word in line.split('/'):\n",
        "        if word==t2:\n",
        "            f1=1;\n",
        "    if f1==1:\n",
        "        for word in line.split():\n",
        "            if word in skills:\n",
        "                if word in dist:\n",
        "                    dist[word]=dist[word]+1\n",
        "                else:\n",
        "                    dist[word]=1\n",
        "print(dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Skill Extraction\n"
      ],
      "metadata": {
        "id": "dvAdCHNshMKO"
      },
      "id": "dvAdCHNshMKO"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjvK_zH_Kf_9",
        "outputId": "e9434291-b39a-4157-d660-34639d918836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-30 15:19:06.504512: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-30 15:19:07.722943: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "id": "fjvK_zH_Kf_9"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "skill_pattern_path = \"/content/jz_skill_patterns.jsonl\""
      ],
      "metadata": {
        "id": "Zmh546voKf9Z"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Zmh546voKf9Z"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "ruler.from_disk(skill_pattern_path)\n",
        "nlp.pipe_names"
      ],
      "metadata": {
        "id": "Q7kbbzq9Kf7A"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Q7kbbzq9Kf7A"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def get_skills(text):\n",
        "    doc = nlp(text)\n",
        "    myset = []\n",
        "    subset = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"SKILL\":\n",
        "            subset.append(ent.text)\n",
        "    myset.append(subset)\n",
        "    return subset\n",
        "\n",
        "\n",
        "def unique_skills(x):\n",
        "    return list(set(x))"
      ],
      "metadata": {
        "id": "BpFDRoOYKf4U"
      },
      "execution_count": null,
      "outputs": [],
      "id": "BpFDRoOYKf4U"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords  # Import the stopwords module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lm = WordNetLemmatizer()\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Download the WordNet resource\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkXyZfyKsxLy",
        "outputId": "d0f9af92-f17e-4993-d743-e6ef93e4478e"
      },
      "id": "QkXyZfyKsxLy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def preprocess_text(Txt):\n",
        "    # Remove special characters, URLs, mentions, and other non-alphanumeric characters\n",
        "    Txt = re.sub('(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt', \" \", Txt)\n",
        "\n",
        "    # Convert to lowercase and split into words\n",
        "    Txt = Txt.lower()\n",
        "    words = Txt.split()\n",
        "\n",
        "    # Lemmatize words and remove stopwords\n",
        "    words = [lm.lemmatize(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
        "\n",
        "    # Join the cleaned words back into a single string\n",
        "    cleaned_text = \" \".join(words)\n",
        "\n",
        "    return cleaned_text\n",
        "cleaned_resume_text = preprocess_text(resumeText)"
      ],
      "metadata": {
        "id": "BZ81IwgNKf2P"
      },
      "execution_count": null,
      "outputs": [],
      "id": "BZ81IwgNKf2P"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "skills = get_skills(cleaned_resume_text)\n",
        "unique_skills_list = unique_skills(skills)\n",
        "print(unique_skills_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pszpRxZxmqTk",
        "outputId": "201b9460-f8e9-4f6b-deb6-e4af0d089b60"
      },
      "id": "pszpRxZxmqTk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['machine learning', 'deep learning', 'library', 'python', 'java', 'wavelet', 'algorithm', 'network architecture']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c3f558",
      "metadata": {
        "id": "d4c3f558"
      },
      "source": [
        "# Recomendation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ec0e4c0",
      "metadata": {
        "id": "5ec0e4c0"
      },
      "outputs": [],
      "source": [
        "''' Cleaning Data:\n",
        "1 URLs,\n",
        "2 hashtags,\n",
        "3 mentions,\n",
        "4 special letters,\n",
        "5 punctuations: '''\n",
        "\n",
        "%%time\n",
        "import re\n",
        "def cleanResume(txt):\n",
        "    cleanText = re.sub('http\\S+\\s', ' ', txt)\n",
        "    cleanText = re.sub('RT|cc', ' ', cleanText)\n",
        "    cleanText = re.sub('#\\S+\\s', ' ', cleanText)\n",
        "    cleanText = re.sub('@\\S+', '  ', cleanText)\n",
        "    cleanText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', cleanText)\n",
        "    cleanText = re.sub(r'[^\\x00-\\x7f]', ' ', cleanText)\n",
        "    cleanText = re.sub('\\s+', ' ', cleanText)\n",
        "    return cleanText\n",
        "\n",
        "resumeText=cleanResume(resumeText)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8985051c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "8985051c",
        "outputId": "7e574069-5ce4-48a9-b85d-4aa6050c890b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-0e2fa5febb75>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Load the trained classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'clf.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mtfidfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'clf.pkl'"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "\n",
        "# words into categorical values\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "\n",
        "\n",
        "# ['Data Science', 'HR', 'Advocate', 'Arts', 'Web Designing',\n",
        "#        'Mechanical Engineer', 'Sales', 'Health and fitness',\n",
        "#        'Civil Engineer', 'Java Developer', 'Business Analyst',\n",
        "#        'SAP Developer', 'Automation Testing', 'Electrical Engineering',\n",
        "#        'Operations Manager', 'Python Developer', 'DevOps Engineer',\n",
        "#        'Network Security Engineer', 'PMO', 'Database', 'Hadoop',\n",
        "#        'ETL Developer', 'DotNet Developer', 'Blockchain', 'Testing'],\n",
        "#       dtype=object)\n",
        "\n",
        "\n",
        "\n",
        "# Prediction System\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Load the trained classifier\n",
        "clf = pickle.load(open('clf.pkl', 'rb'))\n",
        "tfidfd = pickle.load(open('tfidf.pkl', 'rb'))\n",
        "\n",
        "# Clean the input resume\n",
        "cleaned_resume = cleanResume(resumeText)\n",
        "\n",
        "# Transform the cleaned resume using the trained TfidfVectorizer\n",
        "input_features = tfidfd.transform([cleaned_resume])\n",
        "\n",
        "# Make the prediction using the loaded classifier\n",
        "prediction_id = clf.predict(input_features)[0]\n",
        "\n",
        "# Map category ID to category name\n",
        "\n",
        "category_mapping = {\n",
        "    15: {\n",
        "        \"Job Category\": \"Java Developer\",\n",
        "        \"Required Skills\": ['Java language', 'Spring Hibernate', 'RESTful API', 'Maven', 'Jenkins', 'JUnit']\n",
        "    },\n",
        "    23: {\n",
        "        \"Job Category\": \"Testing\",\n",
        "        \"Required Skills\": ['Software Testing', 'Test Automation', 'Quality Assurance', 'Load Testing', 'Selenium', 'Bug Tracking']\n",
        "    },\n",
        "    8: {\n",
        "        \"Job Category\": \"DevOps Engineer\",\n",
        "        \"Required Skills\": ['DevOps Tools', 'CI/CD', 'Containerization', 'Infrastructure as Code', 'Docker', 'Kubernetes']\n",
        "    },\n",
        "    20: {\n",
        "        \"Job Category\": \"Python Developer\",\n",
        "        \"Required Skills\": ['Python', 'Web Frameworks', 'Data Analysis', 'Django', 'Flask', 'Pandas']\n",
        "    },\n",
        "    24: {\n",
        "        \"Job Category\": \"Web Designing\",\n",
        "        \"Required Skills\": ['HTML', 'CSS', 'JavaScript', 'Responsive Design', 'UI/UX Design', 'Front-end Frameworks']\n",
        "    },\n",
        "    12: {\n",
        "        \"Job Category\": \"HR\",\n",
        "        \"Required Skills\": ['Recruitment', 'HR Policies', 'Employee Relations', 'Performance Management', 'HR Software', 'Training and Development']\n",
        "    },\n",
        "    13: {\n",
        "        \"Job Category\": \"Hadoop\",\n",
        "        \"Required Skills\": ['Hadoop Ecosystem', 'Big Data Processing', 'MapReduce', 'Spark', 'Hive', 'HBase']\n",
        "    },\n",
        "    3: {\n",
        "        \"Job Category\": \"Blockchain\",\n",
        "        \"Required Skills\": ['Blockchain Technology', 'Smart Contracts', 'Cryptocurrency', 'Ethereum', 'Hyperledger', 'Consensus Algorithms']\n",
        "    },\n",
        "    10: {\n",
        "        \"Job Category\": \"ETL Developer\",\n",
        "        \"Required Skills\": ['ETL Tools', 'Data Transformation', 'Data Warehousing', 'ETL Frameworks', 'Data Integration', 'Data Quality']\n",
        "    },\n",
        "    18: {\n",
        "        \"Job Category\": \"Operations Manager\",\n",
        "        \"Required Skills\": ['Operations Management', 'Process Optimization', 'Supply Chain', 'Project Management', 'Lean Six Sigma', 'Business Process Modeling']\n",
        "    },\n",
        "    6: {\n",
        "        \"Job Category\": \"Data Science\",\n",
        "        \"Required Skills\": ['Data Analysis', 'Machine Learning', 'Statistics', 'Deep Learning', 'Data Visualization', 'Natural Language Processing']\n",
        "    },\n",
        "    22: {\n",
        "        \"Job Category\": \"Sales\",\n",
        "        \"Required Skills\": ['Sales Techniques', 'Customer Relationship Management', 'Market Research', 'Negotiation Skills', 'Sales Analytics', 'Lead Generation']\n",
        "    },\n",
        "    16: {\n",
        "        \"Job Category\": \"Mechanical Engineer\",\n",
        "        \"Required Skills\": ['Mechanical Design', 'CAD', 'Thermodynamics', 'Finite Element Analysis', 'Product Lifecycle Management', 'Manufacturing Processes']\n",
        "    },\n",
        "    1: {\n",
        "        \"Job Category\": \"Arts\",\n",
        "        \"Required Skills\": ['Artistic Creativity', 'Visual Design', 'Art History', 'Illustration', 'Digital Art', 'Photography']\n",
        "    },\n",
        "    7: {\n",
        "        \"Job Category\": \"Database\",\n",
        "        \"Required Skills\": ['Database Management', 'SQL', 'Data Modeling', 'Database Administration', 'NoSQL Databases', 'Database Tuning']\n",
        "    },\n",
        "    11: {\n",
        "        \"Job Category\": \"Electrical Engineering\",\n",
        "        \"Required Skills\": ['Electrical Design', 'Circuit Analysis', 'Electromagnetism', 'Power Electronics', 'Control Systems', 'Digital Signal Processing']\n",
        "    },\n",
        "    14: {\n",
        "        \"Job Category\": \"Health and fitness\",\n",
        "        \"Required Skills\": ['Fitness Training', 'Nutrition', 'Physical Therapy', 'Sports Medicine', 'Exercise Physiology', 'Rehabilitation']\n",
        "    },\n",
        "    19: {\n",
        "        \"Job Category\": \"PMO\",\n",
        "        \"Required Skills\": ['Project Management', 'Process Governance', 'Stakeholder Communication', 'Risk Management', 'Agile Methodology', 'Program Management']\n",
        "    },\n",
        "    4: {\n",
        "        \"Job Category\": \"Business Analyst\",\n",
        "        \"Required Skills\": ['Business Analysis', 'Requirements Gathering', 'Data Modeling', 'Business Process Improvement', 'UML', 'Use Case Analysis']\n",
        "    },\n",
        "    9: {\n",
        "        \"Job Category\": \"DotNet Developer\",\n",
        "        \"Required Skills\": ['.NET Framework', 'C# Programming', 'ASP.NET', 'MVC Framework', 'Entity Framework', 'Web API Development']\n",
        "    },\n",
        "    2: {\n",
        "        \"Job Category\": \"Automation Testing\",\n",
        "        \"Required Skills\": ['Test Automation Tools', 'Scripting', 'Selenium', 'Load Testing', 'API Testing', 'Performance Testing']\n",
        "    },\n",
        "    17: {\n",
        "        \"Job Category\": \"Network Security Engineer\",\n",
        "        \"Required Skills\": ['Network Security', 'Firewall Configuration', 'Intrusion Detection', 'Security Policies', 'Penetration Testing', 'Security Auditing']\n",
        "    },\n",
        "    21: {\n",
        "        \"Job Category\": \"SAP Developer\",\n",
        "        \"Required Skills\": ['SAP ERP', 'ABAP Programming', 'SAP HANA', 'SAP Fiori', 'SAP Integration', 'SAP Security']\n",
        "    },\n",
        "    5: {\n",
        "        \"Job Category\": \"Civil Engineer\",\n",
        "        \"Required Skills\": ['Civil Engineering Design', 'Structural Analysis', 'Construction Management', 'Geotechnical Engineering', 'Project Planning', 'Environmental Engineering']\n",
        "    },\n",
        "    0: {\n",
        "        \"Job Category\": \"Advocate\",\n",
        "        \"Required Skills\": ['Legal Advocacy', 'Courtroom Litigation', 'Legal Research', 'Legal Writing', 'Client Counseling', 'Mediation']\n",
        "    },\n",
        "}\n",
        "category_name = category_mapping.get(prediction_id, \"Unknown\")\n",
        "\n",
        "print(\"Predicted Category:\", category_name)\n",
        "print(prediction_id)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}